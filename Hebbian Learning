# 1. Load Data
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchvision
train_dataset = torchvision.datasets.MNIST(root='./data', 
                                           train=True, 
                                           download=True)
                                           
# Change data type : torch.Tensor -> numpy array
X = train_dataset.train_data.numpy()
y_label = train_dataset.train_labels.numpy()
print('Data size: ' + str(X.shape))
print('Data size: ' + str(y_label.shape))

Nc=10
Ns, height, width = X.shape #60000, 28, 28
N = height * width #784
M = X.reshape(-1,784)
M = M/255

# 2. Function: Visualize Weights
def draw_weights(synapses, Kx, Ky):
    yy=0
    HM=np.zeros((30*Ky,30*Kx))
    for y in range(Ky):
        for x in range(Kx):
            HM[y*30:(y+1)*30-2,x*30:(x+1)*30-2]=synapses[yy,:].reshape(28,28)
            yy += 1
    plt.clf()
    nc=np.amax(np.absolute(HM))
    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)
    fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])
    plt.axis('off')
    fig.canvas.draw()
    
# 3. Hyperparameters
eps0=1e-3
# eps0=2    # learning rate
Kx=10
Ky=10
hid=Kx*Ky    # number of hidden units that are displayed in Ky by Kx array
m=0.0
sigma=1.0
Nep=1    # number of epochs
Num=100      # size of the minibatch
prec=1e-30
gamma0=0.002 #Strength of Inhibitory connection

# 4. Hebbian Learning (Oja's Rule)
%matplotlib inline
%matplotlib notebook
fig=plt.figure(figsize=(12,12))

#weight 초기화 (100,784)
synapses = np.random.normal(m, sigma, (hid, N))
print('synapses'+str(synapses[10:30,0]))
# v = np.random.normal(m,sigma,(hid,Num))#weight of Inhibitory connections
y = np.random.normal(m,sigma,(hid,Num))
print('y'+str(y[1:4,0]))

for nep in range(Nep): #epoch
    print('epoch'+str(nep))
    eps = eps0*(1-nep/Nep) #learning rate decrease
    gamma = gamma0*(1-nep/Nep)
    v = np.triu(np.random.uniform(-0.1, 0.1, size=(100,100)))
    np.fill_diagonal(v, 0.0)
    
    M=M[np.random.permutation(Ns),:]
    y_p = np.zeros((100, 100))
    
    for i in range(Ns//Num):  #minibatch 100, cycle 600Ns//Num
        print('minibatch'+str(i))
        gamma1 = gamma*(1-i/200)#Ns//Num
        inputs = np.transpose(M[i*Num:(i+1)*Num,:]) #extract 100 image from M (784,100)
        
#1.y    (Inhibitory connection 초기화 5번에 한번)
        y = np.dot(synapses,inputs)+np.multiply(v,y_p)
#                 #y (100,784)*(784,100) + (100,100) = (100,100)
        for _ in range(2):
            y = np.dot(synapses, inputs)+np.multiply(v,y)
        print(y.shape)
        y1=y
        y = np.mean(y, axis = 1)
        y2 = y**2
        print('y'+str(y[1:4]))
        
#2.dW
        dw = eps*(np.multiply(np.tile(y.reshape(y.shape[0],1),(1,N)), inputs.T)- np.multiply(np.tile(y2.reshape(y.shape[0],1),(1,N)), synapses))
        dw /=np.linalg.norm(dw,axis=0).reshape((1, 784))#수렴 후보
        print('dw'+str(dw[1:4]))

        
#3.dv   np.multiply(np.tile(y.reshape(y.shape[0],1),(1,N)), inputs.T)
        dv = -1*gamma*np.multiply(np.tile(y.reshape(100,1),(1,100)),np.tile(y.reshape(1,100),(100,1))) 
        dv /=np.linalg.norm(dv)#숫자 계속 변동, 흰배경, 발산x
        v += dv
        print('dv'+str(dv[0:5,30]))
        print('v'+str(v[0:5,30]))
        v = 1/100*np.triu(v.T)
        np.fill_diagonal(v, 0.0)

#update synapse        
        nc=np.amax(np.absolute(dw))
        synapses += np.true_divide(dw,1)
        print('synapses'+str(synapses[10:20,0]))
        print('synapses'+str(synapses[10:20,24]))
        synapses /= np.linalg.norm(synapses, axis=0).reshape((1, 784))#흰배경,가운데 동그랗게 됨,


        draw_weights(synapses, Kx, Ky)
